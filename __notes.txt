CONTRAST

-tradeoff between over-averaging fine detail and under-averaging non-photographic artifacts and flaws
  -solution: dynamically adjust sigma 
    -measure cosine distance between raw brightness histogram and blurred one (like a high blur)
    -if that distance is very high, lower sigma a bunch
    -if it's not, let sigma go high
    -turns out that doesn't work great
  -better solution: check which prints lose their low blacks the most by blurring
    -grab the lowest, say, 10,000 pixels
    -take median or mean
    -see how the proportion lower than this mean changes after blurring
    -if you're losing tons of pixels below the 'mean black', that's a red flag
    -in such case, sigma should be lower (so you don't lose as many)
  -even better solution: just take the median of the K darkest pixels
    -this performs better than the other two methods
    -assuming you set K intelligently, you will 'average out' artifacts
      -because although they will fall in K, they will be medianed-out
    -and this allows the high-contrast-but-delicate prints to shine
    -this also gets at something important: we are more interested in global patterns
      -the old method was too focused on two extreme regions
    -nice thing is that you can repeat this with saturation
      -find the K lightest pixels, and take the median of their saturation


-also a theretical question about whether the absolute largest diff is what we want
-maybe 'over' averaging is actually appropriate?
 -although for a fine detail photo, it's more that you'd want the "top ten" diffs, say
 -not to wipe out all diffs with a big sigma
-also, 'fading' is really about losing your black blacks
 -I mean, it's not as if you can 'fade' by having your whites go gray
 -so maybe we should just be measuring the 'top 10 blacks'?

-interesting issue here too about how you validate an algorithm
 -in one sense, best thing to do is just gather human judgments and match them with NN
 -nobody will care that the algorithm is opaque, because it is obtained in a legible way:
  -it's MATCHING something we understand
 -when you try to analytically design your own, that's when people protest




STORY

1. traditional method: find dmax and dmin by eye, measure with spectrophotometer
2. I copy that method in code, finding a small region to match what you'd find by eye
   -worked great for CdV, all of which had large contiguous regions of dark
3. I then improve on the speed by realizing that I was basically doing Gaussian blurring anyway, just much more slowly
4. I then looked at some results in the salted paper print data, and realized there was a problem: fine details were getting clobbered by the Gaussian, even when the photo silver was dark
5. Then I thught --- adjust the sigma for those
   -how to pick sigma?
     a. cosine distance between raw and blurred; if high, then fine detail; flaws and artifacts too few to make a big diff in cosine dist
        -might have worked ok, but computationally expensive, and still very focused on single blurred pixels
     b. pick sigma by seeing if a print loses its low blacks from blurring
        -if you do, then it's probably fine detail
        -same issue as before --- you're still betting the whole farm on one blurred pixel, and praying you've chosen the right sigma
        -the blurring alters the print in ways that can be hard to predict, and then you're hoping the algorithm finds a diagnostic spot
6. But what does 'diagnostic' mean? Is that spot standing in for something else, or do we care about the extrema?
   -I think we don't: we care about the pattern
   -If a print has lots of deep blacks that aren't flaws or artifacts, then that should score high for condition
   -we need to look at MANY points
   -this is not feasible with the spectrophotometer; it's trivially easy with computer vision
7. So, we find the k deepest pixels, and take the median
   -now, if you have LOTS of dark flaws (and some do!), there is really nothing to be done
     -we'd need a much more sophisticated way of picking that out --- maybe if the collection is discontinuous with the rest of the print, e.g.
   -for now, we'll not worry too much about that, we will try to crop out bad spots at the edges, where they are more common
   -and the advantage of this approach:
     -we are not altering the image at all
     -we count the flaws and artifacts, but if they are anomalous, they have little impact, because we take the median
     -and we take enough pixels that we are likely to swamp out the flaws
     -this rather than rolling the dice on a blur and hoping we don't end up with a flaw
       -in this method, we are likely to have both flaws AND genuine blacks, and the median computation should help us focus on the latter
     -moreover, we are no longer focusing on dmin at all when we look at 'contrast', because we don't really care about contrast
       -we care about 'lowtone'
       -save caring about dmin for the saturation bit



K

(0.004%) 10: too small; mostly flaws
(0.04%) 100: better, but still probably too far weighted to the flaws, and we have LOTS of room to add pixels before the fine details are spent
(0.4%) 1000: small flaws are swamped out, but big flaws might still dominate; if you go this low, try to crop out flaws; still room to grow
(4%) 10000: too far --- not necessary, and possibly exhausting fine detail images

* (1%): 2621: just about right
   -I think there is little difference between 1000,2000,3000, and others, but by the time you get to 10,000, it's too many
   -one percent is a nice, easy to remember rule
   -but always check how the algorithm is doing! 



CLUSTERING AND GENOME NEIGHBORS

1. Cluster artist in isolation, but in genome-wide space
   -Why?
       -You can't cluster an artist in its own space --- too likely to find phantom distinctions
       -We saw this with Talbot: his gloss values varied from 2-5 GU. That probably doesn't even exceed our measurement error
        -So, if we cluster in a space where one dimension is gloss, normed by these boundaries, we will make distinctions where there are none
       -You need the genome BOUNDS to tell you whether a difference matters or not
       -However, you don't want to cluster an artist together with the genome
        -The genome contains too many items that are irrelevant to a given artist, and we don't want those items exerting pressure on the grouping
        -Moreover, the genome is WAY bigger than any artist collection, and we don't want it to dictate where the cut points are 
         -Clustering in this space is VERY difficult. The data is not clumpy. Thus, clustering algorithms sometimes return weird cut points
          -We don't want those cut points authored by papers that are irrelevant to the artist at hand
       -I've had some success 'clustering' the genome using 3-4 bin edges per axis
        -This is how GEP plots work in Paperbase
        -This creates a lot of clusters, however: right now, Paperbase uses 79
        -Moreover, an artist's body of work is not a random collection of papers across the genome
         -It is a carefully curated shortlist of papers that were available to them
         -It has a discrete causal history
         -Thus, I don't want global cut points to warp this history
         -We have measurement error, and we have irrelevant similarities in this data
         -I want the artist's own physical outputs to be the substrate on which the clustering analysis works
         -It respects the fact that each artist, in their work, is carving out a legible history, one that cuts its own veins in the space 
2. Find genome neighbors for each cluster 
   -use notion of interiority to define neighbor threshold
   -choose neighbor distance target
   -currently, target is the centroid (mean)
   -interiority is defined as "closer than farthest artist member"
3. Genome neighbor search can be thresholded by date or other properties

ISSUES
  -Interiority is probably too permissive now
  -It's possible that finding a small number of neighbors for each artist member is better?
  -But really, the key is getting better artist clusters
  -And once you get them, maybe using a density definition of the centroid, and possibly placing the neighbor threshold closer in 
  -I think what I'm seeing now is that I get somewhat heterogeneous clusters, and then there are SO MANY genome neighbors at least as close as the farthest artist members
  -It's possible taking median instead of mean would help too
  -But really, again, I think the core issue is that I'm getting loose clusters
  -May need to develop a custom way of clustering this data, which is low-dimensional and very diffuse --- lots of the sim space is occupied

ALGORITHMS 
  -We've used k-means and hierarchical clustering algorithms thus far
  -But they have issues: they force items into clusters, for one, even if those items are noisy
  -So, we could use DBSCAN
   -But that has its own issues: one is that you have to tweak the settings A LOT
   -And it tends towards treating everything as noise --- you have to mess with `eps` and `min_samples` to get what you want
   -However, a bigger issue is that it only seems to work really well with clusters of similar density
  -So, an alternative to this is HDBSCAN, which is hierarchical and allows for clusters of different densities
  -But ANOTHER issue is simply that our simspace is not clumpy
   -We can make it look less clumpy with UMAP, but the problem remains: we have low-dimensional data, and it's very easy to see where UMAP has warped our space
   -When you are in higher dimensions, your space will be sparser, almost always, and clustering tends to work well as a way of managing that sparsity
   -But when you have a lower dimensional space that is pretty filled out, you're just not going to get great clusters
   -I don't think the answer is to make it seem artifically clumpy with UMAP
   -I'm not sure what the answer is, however

NORM 
   -For a while, I was always using a raw norm, although in Paperbase, the norms are percentiles
   -It's a little unclear to me which I should use
   -I now lean towards the percentile norms, because the impact of weird outliers is so common and so boring
   -But in any case, this is something to consider
   -One problem with the rank norms is that you also lose interior differences, and the data is thus even harder to cluster

OUTLIERS  
   -Outliers can be interesting, and the density-based clustering algorithms can handle them
   -However, there are legitimate outliers, and illegitimate ones, and you need to find out which are which
   -For us, there are usually several illegitimate roughness outliers that result from bad texture captures
    -The texture algorithm is not the problem here: it does a good job of finding these spurious outliers, actually
    -But usually they result from getting a micrograph at the edge of the photo, and then that hard edge makes it into the std computation
   -We see this in Alvarez-Bravo, which is making it very difficult to cluster the rest of the data
   -So, I will simply have to eliminate those from the analysis, or find a way to slot them into clusters based on their other qualities
   -One thing you can sometimes try is to crop them from the start. Right now, all we need is 768x768

LESSONS FROM EXPERIMENTS
   -UMAP warps the space, and the results are not what you'd want
   -I think it's that our data is too small and low-dimensional, and not clumpy
    -So, if you enforce clumpiness, it will be false  
   -UMAP is unusable for clustering, for neighbor finding, all of it
   -Because our data is not clumpy, I think we need to do NN on individual items
   -Maybe we can build clusters after that?
    -Either by creating a graph representation
    -Or perhaps by looking at neighbor overlap
    -Or just clustering on the points we end up with when we find neighbors 
   -Or, we could just cluster on Lola in some way, and then do NN 
   -Checking a good clustering 
    -Making a minimum set of groupings it should agree with 
     -We could perhaps find these automatically
     -Like if two points share X number of neighbors, they must be in same cluster?
     -Or just manually go through and do some grouping 

NEW CLUSTERING ALGORITHM

-This is an algorithm that would be special to our situation, where we have points we want to cluster, and a background of other points
-The core idea is this:
 -Nearest neighbors is working great in this space, and it's pretty fast (because only 4d)
 -This will always be the case with these collections:
  -Small number of items to cluster 
  -Roughly 5500 background points 
  -4d
-We want our clustering algorithm to be easy to understand, and to follow an explicitly reasonable path
 -We want these good neighbor relations --- from item to genome item --- to be what dictates our clustering
  -Why not just cluster using Lola plus genome?
   -We want the neighbor relations to be the crux, because we don't care about clusters of papers in general
    -We care about the ones gathered around our collection items 
  -Okay, but why use the genome points at all at this stage? Why not just bring them in later, as friends?
   -They can help guide us between collection items that vary a bit because of natural paper variation or measurement variance
   -The genome is vast, and gives us islands of density 
   -The collections are small, and can either lack important connective tissue, or represent too small a sample of paper types
-The way to understand this approach is basically:
 -We ARE clustering with Lola+Genome, but we are doing it in a way guided by Lola
  -We are allowing Lola points to "invite" parts of the genome to the clustering party
  -And we are using explicit definitions of invitee relations to decide who gets to "sit together" (belong to the same group) 
  -These explicit definitions might in fact be approximated by existing algorithms, but our approach is more interpretable
  -It is understandable to anyone who would understand the analogy case from party planning 
  -You wouldn't seat guests at wedding tables by reference to a mass of uninvited guests who won't be attending
   -You don't want irrelevant people to be dictating your seating arrangement
-Parameters
 -Quick setup: a "member" is a Lola print; a "guest" is a genome paper
 -I haven't decided yet exactly how this should work
  -number of guests per member 
  -maximum member-guest distance (like, 'family only')
-Algorithm 
 -Some possibilities
  -every member starts with k guests, "with replacement", meaning members can invite the same guest
   -find the # clusters and cluster definitions that minimize the number of unique guests (since some will be shared)
    -assuming that guests become 'unique' only within a cluster 
     -otherwise, I think, the cluster would play no role in unique-guest-minimization
    -and assuming that members can only sit at the same table together if they share at least one guest
   -guests invited by multiple members get marked as such, and their metadata properties get higher billing
  -ok great, now how does this actually compute?
   -this is more challenging, but probably can be solved with graph theory
    -because, we start by taking a continous space and turning it into an "invite" graph 

-another advantage of this approach over simply clustering on lola + genome
 -we can now identify prints that cannot invite a single guest: no guest close enough
 -that's an interesting case we would have ignored before

So, outputs:
-clustering
-genome guests, and noting which guests are multiply invited
-which tables are linked
-which members can invite no guests
-which members can invite guests but cannot share a table with any other members 

Another point about justifying how we are doing this:
-Someone might ask, why let the genome decide the clusters (via guest-sharing maximization --- or we might say, unique guest minimization)?
 -Because the clusters are only important insofar as we use them to characterize the genome neighbors of a group of prints
 -Once we've done the NN on the genome, the clusters are really just a structured way to talk about the print members
 -And what we want to know is: how are they connected to the genome?
 -Moreover, if two prints are very very close together, they will have all the same genome guests anyway, since all are in the same space
 -The guest-sharing thing is a device that gives us a connection to the reference collection, which we care about
  -And it also gives us a discrete way of deciding whether two print members belong together in a cluster
 -Moreover, again, there is no clustering algorithm in existence that delivers what we are doing here
  -We have two kinds of noise:
   -no genome guests
   -no guests shared with any other members 
 -The nature of our problem involves two sets of points, members and guests, and they have distinct ways of mattering
  -We can't just use some existing algorithm

LESSONS 

Well, that didn't work --- at least, not how I planned. But I learned a bunch of lessons.
-First, the core problem was that I had set myself to solve a combinatorics / set theory problem that requires a massive amount of computation 
-And every conceivable way of making the problem easier involved adverting to the numerical feature space information I had discarded
-I suppose that I changed the nature of the problem from numerical to categorical because I wanted to place certain constraints on solutions
 -I wanted to respect the print/reference duality of our data
 -I wanted to have lots of interesting categories of prints, like "lonely", "outsider"
-I also wanted the final results to be both interpretable and defensible, and that's tough to do with, say, DBSCAN
-But ultimately, the problem is too expensive to solve with combinatorics
 -I'm sure there is an optimal solution, but we can get very close, I'm sure, just using DBSCAN and a few constraints 

ASIDE
-I thought I'd be able to develop a new clustering algorithm 
-But I had never thought that much about clustering, really 
-More, perhaps, than the average fella working on supervised models, but not beyond thinking about simspaces and k-means vs hierarchical 
-Part of it was that I used to use clustering algorithms on _image data_, and it's VERY EASY to check if your image clusters are bad 
 -So, clustering is VERY EFFECTIVE for images, since they are crazy high dimensional and the space is clumpy 
 -AND it is very easy to CHECK how it's doing, because although images are very high-dimensional, our visual systems can read them instantly
-Clustering with data that is not like this is much tougher 
 -Our data is low-dimensional and not clumpy 
 -And it's not so easy to check 


NEW APPROACH: DBSCAN w/ CONSTRAINTS

1. Start by choosing a _d_ that will create an intracluster distance limit that reaches our measurement errors at least 
   -the idea is that we want to use a domain-relevant notion of 'different paper', and our criterion can't be stricter than our instruments can detect 
   -We do this by: 
    -choosing _d_ (maybe start with 0.05)
    -finding pairs of items in the data that sit at this distance, and looking at their raw measurement diffs
    -Checking if these exceed our measurement error 
    -If not, increase _d_  AT LEAST until we've reached our measurement error in every dimension 
2. Use _d_ to then have each member invite guests 
3. Members without guests are "lonelies", make a note of them but remove from data 
4. Members that share no guests with any other members are "outsiders", make note, remove 
5. Run DBSCAN on what remains (non-lonely, insider members only)
   -always keep min_samples at 2, since we will have clusters that small 
   -drive eps as high as possible while still keeping an "allowable tabling" (with d = 0.07, my eps is ~0.03)
    -this means every member of a cluster must share at least one guest with every other member at the table
6. Check guest collapse
7. Check whether any guests are shared across clusters --- shouldn't be
   -The idea is that we don't want any clusters that contain points as far apart as with points in other clusters 
    -So, clusters need to be both dense and well-separated from other clusters 
    



     

