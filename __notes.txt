CONTRAST

-tradeoff between over-averaging fine detail and under-averaging non-photographic artifacts and flaws
  -solution: dynamically adjust sigma 
    -measure cosine distance between raw brightness histogram and blurred one (like a high blur)
    -if that distance is very high, lower sigma a bunch
    -if it's not, let sigma go high
    -turns out that doesn't work great
  -better solution: check which prints lose their low blacks the most by blurring
    -grab the lowest, say, 10,000 pixels
    -take median or mean
    -see how the proportion lower than this mean changes after blurring
    -if you're losing tons of pixels below the 'mean black', that's a red flag
    -in such case, sigma should be lower (so you don't lose as many)
  -even better solution: just take the median of the K darkest pixels
    -this performs better than the other two methods
    -assuming you set K intelligently, you will 'average out' artifacts
      -because although they will fall in K, they will be medianed-out
    -and this allows the high-contrast-but-delicate prints to shine
    -this also gets at something important: we are more interested in global patterns
      -the old method was too focused on two extreme regions
    -nice thing is that you can repeat this with saturation
      -find the K lightest pixels, and take the median of their saturation


-also a theretical question about whether the absolute largest diff is what we want
-maybe 'over' averaging is actually appropriate?
 -although for a fine detail photo, it's more that you'd want the "top ten" diffs, say
 -not to wipe out all diffs with a big sigma
-also, 'fading' is really about losing your black blacks
 -I mean, it's not as if you can 'fade' by having your whites go gray
 -so maybe we should just be measuring the 'top 10 blacks'?

-interesting issue here too about how you validate an algorithm
 -in one sense, best thing to do is just gather human judgments and match them with NN
 -nobody will care that the algorithm is opaque, because it is obtained in a legible way:
  -it's MATCHING something we understand
 -when you try to analytically design your own, that's when people protest




STORY

1. traditional method: find dmax and dmin by eye, measure with spectrophotometer
2. I copy that method in code, finding a small region to match what you'd find by eye
   -worked great for CdV, all of which had large contiguous regions of dark
3. I then improve on the speed by realizing that I was basically doing Gaussian blurring anyway, just much more slowly
4. I then looked at some results in the salted paper print data, and realized there was a problem: fine details were getting clobbered by the Gaussian, even when the photo silver was dark
5. Then I thught --- adjust the sigma for those
   -how to pick sigma?
     a. cosine distance between raw and blurred; if high, then fine detail; flaws and artifacts too few to make a big diff in cosine dist
        -might have worked ok, but computationally expensive, and still very focused on single blurred pixels
     b. pick sigma by seeing if a print loses its low blacks from blurring
        -if you do, then it's probably fine detail
        -same issue as before --- you're still betting the whole farm on one blurred pixel, and praying you've chosen the right sigma
        -the blurring alters the print in ways that can be hard to predict, and then you're hoping the algorithm finds a diagnostic spot
6. But what does 'diagnostic' mean? Is that spot standing in for something else, or do we care about the extrema?
   -I think we don't: we care about the pattern
   -If a print has lots of deep blacks that aren't flaws or artifacts, then that should score high for condition
   -we need to look at MANY points
   -this is not feasible with the spectrophotometer; it's trivially easy with computer vision
7. So, we find the k deepest pixels, and take the median
   -now, if you have LOTS of dark flaws (and some do!), there is really nothing to be done
     -we'd need a much more sophisticated way of picking that out --- maybe if the collection is discontinuous with the rest of the print, e.g.
   -for now, we'll not worry too much about that, we will try to crop out bad spots at the edges, where they are more common
   -and the advantage of this approach:
     -we are not altering the image at all
     -we count the flaws and artifacts, but if they are anomalous, they have little impact, because we take the median
     -and we take enough pixels that we are likely to swamp out the flaws
     -this rather than rolling the dice on a blur and hoping we don't end up with a flaw
       -in this method, we are likely to have both flaws AND genuine blacks, and the median computation should help us focus on the latter
     -moreover, we are no longer focusing on dmin at all when we look at 'contrast', because we don't really care about contrast
       -we care about 'lowtone'
       -save caring about dmin for the saturation bit



K

(0.004%) 10: too small; mostly flaws
(0.04%) 100: better, but still probably too far weighted to the flaws, and we have LOTS of room to add pixels before the fine details are spent
(0.4%) 1000: small flaws are swamped out, but big flaws might still dominate; if you go this low, try to crop out flaws; still room to grow
(4%) 10000: too far --- not necessary, and possibly exhausting fine detail images

* (1%): 2621: just about right
   -I think there is little difference between 1000,2000,3000, and others, but by the time you get to 10,000, it's too many
   -one percent is a nice, easy to remember rule
   -but always check how the algorithm is doing! 



CLUSTERING AND GENOME NEIGHBORS

1. Cluster artist in isolation, but in genome-wide space
2. Find genome neighbors for each cluster 
   -use notion of interiority to define neighbor threshold
   -choose neighbor distance target
   -currently, target is the centroid (mean)
   -interiority is defined as "closer than farthest artist member"
3. Genome neighbor search can be thresholded by date or other properties

    ISSUES
      -Interiority is probably too permissive now
      -It's possible that finding a small number of neighbors for each artist member is better?
      -But really, the key is getting better artist clusters
      -And once you get them, maybe using a density definition of the centroid, and possibly placing the neighbor threshold closer in 
      -I think what I'm seeing now is that I get somewhat heterogeneous clusters, and then there are SO MANY genome neighbors at least as close as the farthest artist members
      -It's possible taking median instead of mean would help too
      -But really, again, I think the core issue is that I'm getting loose clusters
      -May need to develop a custom way of clustering this data, which is low-dimensional and very diffuse --- lots of the sim space is occupied





     

